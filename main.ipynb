{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c224961",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef8af774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af4e11c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61eb137c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
      "hello there\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda x : [stoi[i] for i in x]\n",
    "decode = lambda x : \"\".join(itos[i] for i in x)\n",
    "print(encode(\"hello there\"))\n",
    "print(decode(encode(\"hello there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac6b8dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dde4ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61d1bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(sample):\n",
    "    data = train_data if sample == \"train\" else val_data\n",
    "    ix = torch.randint(len(data)-block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1105c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([24]) target is 43\n",
      "When input is tensor([24, 43]) target is 58\n",
      "When input is tensor([24, 43, 58]) target is 5\n",
      "When input is tensor([24, 43, 58,  5]) target is 57\n",
      "When input is tensor([24, 43, 58,  5, 57]) target is 1\n",
      "When input is tensor([24, 43, 58,  5, 57,  1]) target is 46\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46]) target is 43\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) target is 39\n",
      "When input is tensor([44]) target is 53\n",
      "When input is tensor([44, 53]) target is 56\n",
      "When input is tensor([44, 53, 56]) target is 1\n",
      "When input is tensor([44, 53, 56,  1]) target is 58\n",
      "When input is tensor([44, 53, 56,  1, 58]) target is 46\n",
      "When input is tensor([44, 53, 56,  1, 58, 46]) target is 39\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39]) target is 58\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) target is 1\n",
      "When input is tensor([52]) target is 58\n",
      "When input is tensor([52, 58]) target is 1\n",
      "When input is tensor([52, 58,  1]) target is 58\n",
      "When input is tensor([52, 58,  1, 58]) target is 46\n",
      "When input is tensor([52, 58,  1, 58, 46]) target is 39\n",
      "When input is tensor([52, 58,  1, 58, 46, 39]) target is 58\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58]) target is 1\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) target is 46\n",
      "When input is tensor([25]) target is 17\n",
      "When input is tensor([25, 17]) target is 27\n",
      "When input is tensor([25, 17, 27]) target is 10\n",
      "When input is tensor([25, 17, 27, 10]) target is 0\n",
      "When input is tensor([25, 17, 27, 10,  0]) target is 21\n",
      "When input is tensor([25, 17, 27, 10,  0, 21]) target is 1\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1]) target is 54\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) target is 39\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(\"train\")\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        inp = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {inp} target is {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e87c8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65]) tensor(5.0364, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "lfJeukRuaRJKXAYtXzfJ:HEPiu--sDioi;ILCo3pHNTmDwJsfheKRxZCFs\n",
      "lZJ XQc?:s:HEzEnXalEPklcPU cL'DpdLCafBheH\n",
      "\n",
      "Training starts..\n",
      "4.873950481414795\n",
      "4.295483112335205\n",
      "4.587007522583008\n",
      "4.13039493560791\n",
      "4.277191638946533\n",
      "4.198069095611572\n",
      "3.7820565700531006\n",
      "3.751546621322632\n",
      "3.958277463912964\n",
      "3.815798282623291\n",
      "3.5388879776000977\n",
      "3.47265362739563\n",
      "3.478971481323242\n",
      "3.308586835861206\n",
      "3.5024595260620117\n",
      "3.5213165283203125\n",
      "3.2738354206085205\n",
      "2.8871004581451416\n",
      "2.7784464359283447\n",
      "3.138400077819824\n",
      "3.103850841522217\n",
      "2.9894638061523438\n",
      "2.755727529525757\n",
      "2.6786446571350098\n",
      "2.6702511310577393\n",
      "2.8958916664123535\n",
      "2.62581467628479\n",
      "2.770490884780884\n",
      "2.591963052749634\n",
      "2.7645633220672607\n",
      "2.777736186981201\n",
      "2.386279582977295\n",
      "2.7074332237243652\n",
      "2.4616174697875977\n",
      "2.9699602127075195\n",
      "2.505662202835083\n",
      "2.763460159301758\n",
      "2.492985486984253\n",
      "2.7798407077789307\n",
      "2.212958812713623\n",
      "2.3677852153778076\n",
      "2.656628131866455\n",
      "2.394922971725464\n",
      "2.439276933670044\n",
      "2.563270092010498\n",
      "2.3527371883392334\n",
      "2.640653371810913\n",
      "2.3899827003479004\n",
      "2.7086400985717773\n",
      "2.560225248336792\n",
      "2.2618887424468994\n",
      "2.792232036590576\n",
      "2.7128489017486572\n",
      "2.658879041671753\n",
      "2.558703899383545\n",
      "2.5512003898620605\n",
      "2.50734281539917\n",
      "2.2323317527770996\n",
      "2.6284399032592773\n",
      "2.5179431438446045\n",
      "2.883545398712158\n",
      "2.768280506134033\n",
      "2.1333441734313965\n",
      "2.4959921836853027\n",
      "2.0665738582611084\n",
      "2.6255030632019043\n",
      "2.3588950634002686\n",
      "2.6616761684417725\n",
      "2.4834771156311035\n",
      "2.488511323928833\n",
      "2.251654624938965\n",
      "2.513169527053833\n",
      "2.268378734588623\n",
      "2.5809617042541504\n",
      "2.5348317623138428\n",
      "2.4332427978515625\n",
      "2.73939847946167\n",
      "2.4552998542785645\n",
      "2.1665902137756348\n",
      "2.644301652908325\n",
      "2.5466768741607666\n",
      "2.704251289367676\n",
      "2.5261833667755127\n",
      "2.6905245780944824\n",
      "2.5392889976501465\n",
      "2.5870707035064697\n",
      "2.558823823928833\n",
      "2.039322853088379\n",
      "2.1691737174987793\n",
      "2.3464269638061523\n",
      "2.6450109481811523\n",
      "2.4632043838500977\n",
      "2.1924171447753906\n",
      "2.6389381885528564\n",
      "2.643143653869629\n",
      "2.5047335624694824\n",
      "2.269711494445801\n",
      "2.188382387161255\n",
      "2.161468267440796\n",
      "2.18261456489563\n",
      "2.3438632488250732\n",
      "3.009369134902954\n",
      "2.6871278285980225\n",
      "2.4252967834472656\n",
      "2.146671772003174\n",
      "2.314516067504883\n",
      "2.6995668411254883\n",
      "2.3626930713653564\n",
      "2.5301873683929443\n",
      "2.4358301162719727\n",
      "2.4836151599884033\n",
      "2.3651630878448486\n",
      "2.6833271980285645\n",
      "2.545198440551758\n",
      "2.4363999366760254\n",
      "2.6289947032928467\n",
      "2.4543778896331787\n",
      "2.3527395725250244\n",
      "2.4018566608428955\n",
      "2.4443554878234863\n",
      "2.581127643585205\n",
      "2.4444775581359863\n",
      "2.0528061389923096\n",
      "2.196035146713257\n",
      "2.2112913131713867\n",
      "2.3809328079223633\n",
      "2.769615888595581\n",
      "2.4767534732818604\n",
      "2.5438756942749023\n",
      "2.9998865127563477\n",
      "2.7641866207122803\n",
      "2.3594844341278076\n",
      "2.349958658218384\n",
      "2.5848169326782227\n",
      "2.627063512802124\n",
      "2.425034523010254\n",
      "2.5256638526916504\n",
      "2.530618906021118\n",
      "2.179386615753174\n",
      "2.5236353874206543\n",
      "2.1309802532196045\n",
      "2.4774341583251953\n",
      "2.2372965812683105\n",
      "2.4895997047424316\n",
      "2.5039587020874023\n",
      "2.1674211025238037\n",
      "2.6343939304351807\n",
      "2.5594253540039062\n",
      "2.3913426399230957\n",
      "2.520758628845215\n",
      "2.09462571144104\n",
      "2.3084893226623535\n",
      "2.369871139526367\n",
      "2.4717390537261963\n",
      "2.377499580383301\n",
      "2.124821186065674\n",
      "2.350921392440796\n",
      "2.402526617050171\n",
      "2.515530586242676\n",
      "2.670719861984253\n",
      "2.4705822467803955\n",
      "2.3174185752868652\n",
      "2.6518969535827637\n",
      "2.5053696632385254\n",
      "2.3917276859283447\n",
      "2.3747398853302\n",
      "2.4756920337677\n",
      "2.5291028022766113\n",
      "2.0761332511901855\n",
      "2.318274736404419\n",
      "2.3369035720825195\n",
      "2.699826240539551\n",
      "2.5131373405456543\n",
      "2.588240385055542\n",
      "2.1888492107391357\n",
      "2.675422191619873\n",
      "2.4911653995513916\n",
      "2.0665392875671387\n",
      "2.5322182178497314\n",
      "2.5410385131835938\n",
      "2.4409708976745605\n",
      "2.2917025089263916\n",
      "2.1718668937683105\n",
      "2.46016788482666\n",
      "2.3279051780700684\n",
      "2.225454330444336\n",
      "2.4669768810272217\n",
      "2.5617423057556152\n",
      "2.280510187149048\n",
      "2.52854061126709\n",
      "2.413170099258423\n",
      "2.1864569187164307\n",
      "2.6162428855895996\n",
      "2.417739152908325\n",
      "2.6705868244171143\n",
      "2.4077670574188232\n",
      "2.4394261837005615\n",
      "2.351799488067627\n",
      "2.5882821083068848\n",
      "2.4683752059936523\n",
      "2.6027355194091797\n",
      "2.4358294010162354\n",
      "2.711418390274048\n",
      "2.341301202774048\n",
      "2.2543187141418457\n",
      "2.4375903606414795\n",
      "2.6107444763183594\n",
      "2.5052835941314697\n",
      "2.2215259075164795\n",
      "2.0300395488739014\n",
      "2.3559396266937256\n",
      "2.5702664852142334\n",
      "2.597458600997925\n",
      "2.488664150238037\n",
      "2.3870983123779297\n",
      "2.388190269470215\n",
      "2.712463855743408\n",
      "2.768549680709839\n",
      "2.274850606918335\n",
      "2.350526809692383\n",
      "2.4837241172790527\n",
      "2.4937007427215576\n",
      "2.6450560092926025\n",
      "2.6089298725128174\n",
      "2.58375883102417\n",
      "2.3318283557891846\n",
      "2.322983503341675\n",
      "2.149005651473999\n",
      "2.1873645782470703\n",
      "2.4285993576049805\n",
      "2.3741939067840576\n",
      "2.4830734729766846\n",
      "2.5731873512268066\n",
      "2.220141887664795\n",
      "2.342008352279663\n",
      "2.156435489654541\n",
      "2.4762730598449707\n",
      "2.702754497528076\n",
      "2.486172914505005\n",
      "2.1410796642303467\n",
      "2.4605283737182617\n",
      "2.5794951915740967\n",
      "2.635371446609497\n",
      "2.4371695518493652\n",
      "2.322894334793091\n",
      "2.5273187160491943\n",
      "2.234110116958618\n",
      "2.225929021835327\n",
      "2.3232154846191406\n",
      "2.369565010070801\n",
      "2.3582677841186523\n",
      "2.5282979011535645\n",
      "2.4083046913146973\n",
      "2.550373077392578\n",
      "2.528099775314331\n",
      "2.26826810836792\n",
      "2.5395076274871826\n",
      "2.4513280391693115\n",
      "2.666630983352661\n",
      "2.7484383583068848\n",
      "2.3207554817199707\n",
      "2.2923192977905273\n",
      "2.575571060180664\n",
      "2.5784919261932373\n",
      "2.8524816036224365\n",
      "1.9564698934555054\n",
      "2.2225778102874756\n",
      "2.4989981651306152\n",
      "2.377815008163452\n",
      "2.4154133796691895\n",
      "2.6637954711914062\n",
      "2.50089693069458\n",
      "2.4958224296569824\n",
      "2.534010887145996\n",
      "2.3952808380126953\n",
      "1.9866935014724731\n",
      "2.300861358642578\n",
      "2.147193670272827\n",
      "2.4392240047454834\n",
      "2.1817989349365234\n",
      "2.2575132846832275\n",
      "2.510533332824707\n",
      "2.488551378250122\n",
      "2.6505818367004395\n",
      "2.996523380279541\n",
      "2.379324436187744\n",
      "2.0420644283294678\n",
      "2.449760675430298\n",
      "2.1604690551757812\n",
      "1.9645226001739502\n",
      "2.1893177032470703\n",
      "2.869497060775757\n",
      "2.4672353267669678\n",
      "2.834451198577881\n",
      "2.4544429779052734\n",
      "2.3179264068603516\n",
      "2.605665683746338\n",
      "2.358048915863037\n",
      "2.3907082080841064\n",
      "2.5947954654693604\n",
      "2.516129732131958\n",
      "2.778329372406006\n",
      "2.5723438262939453\n",
      "2.531341791152954\n",
      "2.696268320083618\n",
      "2.4331541061401367\n",
      "2.5897841453552246\n",
      "2.487750768661499\n",
      "2.7062036991119385\n",
      "2.6041970252990723\n",
      "2.459174394607544\n",
      "2.704273223876953\n",
      "2.6725094318389893\n",
      "2.304504156112671\n",
      "2.832674980163574\n",
      "2.6013500690460205\n",
      "2.4539742469787598\n",
      "2.306852102279663\n",
      "2.193294048309326\n",
      "2.551783561706543\n",
      "2.3167736530303955\n",
      "2.489177942276001\n",
      "2.3088696002960205\n",
      "2.2447893619537354\n",
      "2.428402900695801\n",
      "2.5280373096466064\n",
      "2.518338441848755\n",
      "2.369213581085205\n",
      "2.6203770637512207\n",
      "2.4781291484832764\n",
      "2.085547924041748\n",
      "2.3434982299804688\n",
      "2.5167088508605957\n",
      "2.438934803009033\n",
      "2.6659698486328125\n",
      "2.4693822860717773\n",
      "2.390134334564209\n",
      "2.4258687496185303\n",
      "2.0031213760375977\n",
      "2.4868109226226807\n",
      "2.2198550701141357\n",
      "2.64003324508667\n",
      "2.4343349933624268\n",
      "2.4074699878692627\n",
      "2.444828987121582\n",
      "2.3675947189331055\n",
      "2.3674380779266357\n",
      "2.541095733642578\n",
      "2.166496515274048\n",
      "2.4600605964660645\n",
      "2.902252197265625\n",
      "2.43613338470459\n",
      "2.628074884414673\n",
      "2.1945841312408447\n",
      "2.5492947101593018\n",
      "2.715519666671753\n",
      "2.4271435737609863\n",
      "2.269761562347412\n",
      "2.3421592712402344\n",
      "2.2177793979644775\n",
      "2.0682849884033203\n",
      "2.359375476837158\n",
      "2.4786746501922607\n",
      "2.3762733936309814\n",
      "2.6134276390075684\n",
      "2.4287021160125732\n",
      "2.515533685684204\n",
      "2.795701026916504\n",
      "2.441964626312256\n",
      "2.2079689502716064\n",
      "2.789766311645508\n",
      "2.5383710861206055\n",
      "2.6252522468566895\n",
      "2.511094093322754\n",
      "2.6476826667785645\n",
      "2.0527689456939697\n",
      "2.286119222640991\n",
      "2.486419916152954\n",
      "2.31084942817688\n",
      "2.481367826461792\n",
      "2.4020578861236572\n",
      "2.532679557800293\n",
      "2.2621560096740723\n",
      "2.4095962047576904\n",
      "2.4619672298431396\n",
      "2.448096990585327\n",
      "2.3457772731781006\n",
      "2.7017199993133545\n",
      "2.8060755729675293\n",
      "2.1578266620635986\n",
      "2.472796678543091\n",
      "2.2565460205078125\n",
      "2.444526433944702\n",
      "2.5185868740081787\n",
      "2.288661003112793\n",
      "2.437628746032715\n",
      "2.4236831665039062\n",
      "2.2190229892730713\n",
      "2.3112683296203613\n",
      "2.552232027053833\n",
      "2.7757279872894287\n",
      "2.360851764678955\n",
      "2.3059990406036377\n",
      "2.463308811187744\n",
      "2.330059766769409\n",
      "2.5893688201904297\n",
      "2.331709861755371\n",
      "2.4460229873657227\n",
      "2.5707569122314453\n",
      "2.736009120941162\n",
      "2.512592077255249\n",
      "2.4580132961273193\n",
      "2.553628921508789\n",
      "2.393665075302124\n",
      "2.5569052696228027\n",
      "2.0712645053863525\n",
      "2.341827392578125\n",
      "2.4486448764801025\n",
      "2.1700491905212402\n",
      "2.59871506690979\n",
      "2.558562994003296\n",
      "2.4462265968322754\n",
      "2.4212450981140137\n",
      "2.6458892822265625\n",
      "2.398521900177002\n",
      "2.5455076694488525\n",
      "2.5280263423919678\n",
      "2.6631157398223877\n",
      "2.718863010406494\n",
      "2.6905124187469482\n",
      "2.5989503860473633\n",
      "2.3454582691192627\n",
      "2.600717544555664\n",
      "2.3168420791625977\n",
      "2.3732614517211914\n",
      "2.0978150367736816\n",
      "2.075777530670166\n",
      "2.553372859954834\n",
      "2.373915672302246\n",
      "2.2937088012695312\n",
      "2.4633266925811768\n",
      "2.4098336696624756\n",
      "2.7403767108917236\n",
      "2.2580301761627197\n",
      "2.294322967529297\n",
      "2.355626106262207\n",
      "2.2206552028656006\n",
      "2.748866319656372\n",
      "2.9208264350891113\n",
      "2.707901954650879\n",
      "2.152301788330078\n",
      "2.4221084117889404\n",
      "2.139267921447754\n",
      "2.4761834144592285\n",
      "2.4542534351348877\n",
      "2.471801996231079\n",
      "2.44850754737854\n",
      "2.5825366973876953\n",
      "2.840841054916382\n",
      "2.5489578247070312\n",
      "2.494694471359253\n",
      "2.431241035461426\n",
      "2.5312037467956543\n",
      "2.4011387825012207\n",
      "2.1507749557495117\n",
      "2.8368585109710693\n",
      "2.522500991821289\n",
      "2.5336782932281494\n",
      "2.323148488998413\n",
      "2.701683759689331\n",
      "2.6771111488342285\n",
      "2.5425848960876465\n",
      "2.5276710987091064\n",
      "2.546660900115967\n",
      "2.2088735103607178\n",
      "2.819368600845337\n",
      "2.473447799682617\n",
      "2.726820707321167\n",
      "2.5929665565490723\n",
      "2.5707199573516846\n",
      "2.4585347175598145\n",
      "2.429177761077881\n",
      "2.5303151607513428\n",
      "2.5110220909118652\n",
      "2.7205471992492676\n",
      "2.452671527862549\n",
      "2.6454789638519287\n",
      "2.54459810256958\n",
      "2.2818193435668945\n",
      "2.333432197570801\n",
      "2.5850846767425537\n",
      "2.4728803634643555\n",
      "2.3499066829681396\n",
      "2.418912887573242\n",
      "2.4138760566711426\n",
      "2.3314671516418457\n",
      "2.0799741744995117\n",
      "2.4674155712127686\n",
      "2.6804513931274414\n",
      "2.5641462802886963\n",
      "\n",
      "Training finished!\n",
      "\n",
      "D bed wowneay cllatirkr:\n",
      "ULOFra re t rssce perfok y bl:\n",
      "CHom\n",
      "\n",
      "An if h atso t end mobe?\n",
      "VOr hawilito \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "class BiagramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.tokenEmbeddingTable(idx)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def train(self, n_iter):\n",
    "        optim = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        for _ in range(n_iter):\n",
    "            xb, yb = get_batch(\"train\")\n",
    "            logits, loss = self(xb, yb)\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            print(loss.item()) if _ % 200 == 0 else None\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            new_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, new_idx),dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BiagramModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape, loss)\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "generated_idx = model.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "print(decode(generated_idx))\n",
    "\n",
    "print(\"\\nTraining starts..\")\n",
    "model.train(100000)\n",
    "print(\"\\nTraining finished!\")\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "generated_idx = model.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "print(decode(generated_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5634bc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N om tond haindrghory seed lyosthangheand y t my\n",
      "HA:\n",
      "heemome; se sis, at!\n",
      "IINes h ad uor\n",
      "MInds, ariengo in nt lerrin,\n",
      "S:\n",
      "KINUpy d t: wes.\n",
      "thos ure chiowinde t IXEn n urs lld d ly in;\n",
      "Je a lds omys, TETht pe ghestonour weveg; wsckistow ghettstond, bamy, tathosplistorloteryois oupeear't y ishatwheas me;\n",
      "AURY: ignjowongazehy wf incrus om s s.\n",
      "WANG s thoth her hthen l therefotentishey cado ad kep:\n",
      "DUS:\n",
      "SThe nthilime geloressevis;\n",
      "NDouin ty be thod theathe fol a\n",
      "Four.\n",
      "ICOfin mace g ber d:\n",
      "Ther otheat\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "generated_idx = model.generate(idx, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d97b5",
   "metadata": {},
   "source": [
    "## Self Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ba83b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4409, 0.5591, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2975, 0.3373, 0.3652, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2211, 0.2898, 0.2236, 0.2654, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1832, 0.2163, 0.1954, 0.2437, 0.1614, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1330, 0.2227, 0.1784, 0.2159, 0.1044, 0.1456, 0.0000, 0.0000],\n",
      "         [0.1283, 0.1367, 0.1385, 0.1522, 0.1083, 0.1341, 0.2021, 0.0000],\n",
      "         [0.1064, 0.1332, 0.1265, 0.1445, 0.0940, 0.1200, 0.1231, 0.1524]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4150, 0.5850, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2313, 0.3588, 0.4098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2210, 0.2636, 0.2829, 0.2324, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1771, 0.2003, 0.2343, 0.2048, 0.1834, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1298, 0.1819, 0.2228, 0.1341, 0.1501, 0.1811, 0.0000, 0.0000],\n",
      "         [0.1292, 0.1590, 0.1739, 0.1224, 0.1203, 0.1684, 0.1268, 0.0000],\n",
      "         [0.1095, 0.1434, 0.1297, 0.1340, 0.1056, 0.1316, 0.1134, 0.1328]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5500, 0.4500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2963, 0.3583, 0.3454, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1693, 0.2521, 0.3023, 0.2763, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1557, 0.2057, 0.2796, 0.2175, 0.1414, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1505, 0.1712, 0.2220, 0.1627, 0.1402, 0.1535, 0.0000, 0.0000],\n",
      "         [0.0932, 0.1432, 0.1805, 0.1377, 0.1012, 0.1751, 0.1691, 0.0000],\n",
      "         [0.0951, 0.1293, 0.1443, 0.1232, 0.0885, 0.1391, 0.1521, 0.1283]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6025, 0.3975, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3287, 0.2486, 0.4226, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3289, 0.1413, 0.2042, 0.3256, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1981, 0.1209, 0.1906, 0.2669, 0.2235, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1510, 0.1055, 0.1833, 0.2230, 0.1960, 0.1412, 0.0000, 0.0000],\n",
      "         [0.1336, 0.0655, 0.1059, 0.2171, 0.1487, 0.1250, 0.2042, 0.0000],\n",
      "         [0.1137, 0.0804, 0.1166, 0.1679, 0.1494, 0.0947, 0.1716, 0.1058]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.rand(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "\n",
    "wei = q @ k.transpose(-2,-1)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril==0, value=float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "\n",
    "out = wei @ v\n",
    "\n",
    "print(out.shape)\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8aeee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d732cc88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
